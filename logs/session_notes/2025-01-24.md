# Session: 2025-01-24 - Project Kickoff

## Server Specs Confirmed âœ“
- **CPU**: 72 threads (Dual Xeon E5-2699 v3)
- **RAM**: 126GB
- **Storage**: 2.18TB (HDDStorage) + 237GB (local)
- **Assessment**: Excellent for AI workloads

## Performance Expectations
- **Llama 3.1 8B**: 15-25 tokens/sec (very good)
- **Response time**: 2-4 seconds typical
- **Can run**: Multiple models simultaneously

## Decisions Made
1. Use Ollama (local AI) instead of OpenAI API - Zero cost
2. Use Llama 3.1 8B as primary model
3. Documentation-first approach
4. GitHub for version control

## VM Allocation Plan
- VM 100: PostgreSQL (8GB RAM, 100GB disk)
- VM 101: Ollama AI (64GB RAM, 200GB disk)
- VM 102: DEV Apps (32GB RAM, 100GB disk)

## Completed Today
- [x] Server specs assessment
- [x] Storage setup (2TB available)
- [x] Cleaned up old VMs (100, 101, 103)
- [x] GitHub repo created
- [x] Documentation structure initialized

## Next Session
- [ ] Write VM creation scripts
- [ ] Install Debian on VM 100 (PostgreSQL)
- [ ] Install Debian on VM 101 (Ollama)
- [ ] Document installation process in SETUP.md

## Key Insights
- Hardware is much better than typical home lab
- 126GB RAM allows experimentation with large models
- Documentation-first prevents future confusion
